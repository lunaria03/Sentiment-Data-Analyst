{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Oriza Sativa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>ribet mengganti alamat surel aktif buka bantua...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-14</td>\n",
       "      <td>kecewa aplikasi bintang kali transaksi paylate...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-14</td>\n",
       "      <td>daftar harga langsung token listrik harga tota...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>jagan beli pulsa bualapak top up transfer puls...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>pembelian dibatalkan keterangannya pembeli bat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27530</th>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>costumer terhormat tolong order order cancel k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27531</th>\n",
       "      <td>2019-04-03</td>\n",
       "      <td>membantu banget kadang greget kupon motornya k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27532</th>\n",
       "      <td>2022-03-10</td>\n",
       "      <td>menerima kode otp aktivasi gopay nomor aplikas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27533</th>\n",
       "      <td>2019-02-18</td>\n",
       "      <td>ship membantu raport nilai pelit tip cas mang ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27534</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>tagihan paylater no handphone aktif instal aku...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27535 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               at                                            comment     label\n",
       "0      2023-11-18  ribet mengganti alamat surel aktif buka bantua...  negative\n",
       "1      2023-11-14  kecewa aplikasi bintang kali transaksi paylate...  negative\n",
       "2      2023-11-14  daftar harga langsung token listrik harga tota...  negative\n",
       "3      2023-11-06  jagan beli pulsa bualapak top up transfer puls...  negative\n",
       "4      2023-11-12  pembelian dibatalkan keterangannya pembeli bat...  negative\n",
       "...           ...                                                ...       ...\n",
       "27530  2020-05-18  costumer terhormat tolong order order cancel k...  negative\n",
       "27531  2019-04-03  membantu banget kadang greget kupon motornya k...  negative\n",
       "27532  2022-03-10  menerima kode otp aktivasi gopay nomor aplikas...  negative\n",
       "27533  2019-02-18  ship membantu raport nilai pelit tip cas mang ...  positive\n",
       "27534  2020-03-20  tagihan paylater no handphone aktif instal aku...  negative\n",
       "\n",
       "[27535 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import Data\n",
    "df = pd.read_csv(\"C:/Kuliah/Bangkit/Capstone/Sentiment/data_final.csv\")\n",
    "\n",
    "# Select specific columns using double square brackets\n",
    "df = df[['at',\"comment\", \"label\"]]\n",
    "\n",
    "# Cut the last 10 characters in each cell of the specified column\n",
    "df['at'] = df['at'].apply(lambda x: x[:-9] if len(x) > 10 else '')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'label' is the column containing 'positive' and 'negative'\n",
    "negative_rows = df[df['label'] == 'negative']\n",
    "\n",
    "# Keep up to 10471 rows of 'negative' and all rows of 'positive'\n",
    "df = pd.concat([negative_rows.head(10254), df[df['label'] != 'negative']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "negative    10254\n",
      "positive    10254\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'label' is the column containing 'positive' and 'negative'\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['comment'].values\n",
    "labels = df['label'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUAT TEST AJA JGN DI RUN\n",
    "\n",
    "vocab_size = 15000\n",
    "oov_tok = ''\n",
    "embedding_dim = 50\n",
    "max_length = 200\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "\n",
    "# Convert test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "\n",
    "# Perform PCA to estimate embedding dimension\n",
    "#embedding_dim_candidate = 50  # You can start with a value and adjust as needed\n",
    "pca = PCA(n_components=embedding_dim)\n",
    "pca_result = pca.fit_transform(train_padded)\n",
    "\n",
    "# Plot the explained variance ratio to help choose an appropriate dimension\n",
    "plt.plot(range(1, embedding_dim + 1), pca.explained_variance_ratio_)\n",
    "plt.title('Explained Variance Ratio by Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000 # choose based on statistics\n",
    "oov_tok = '<OOV>'\n",
    "embedding_dim = 70\n",
    "max_length = 200 # choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "# Ensure all elements in train_sentences are strings\n",
    "train_sentences = [str(sentence) for sentence in train_sentences]\n",
    "test_sentences = [str(sentence) for sentence in test_sentences]\n",
    "\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length,truncating=trunc_type)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length,truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras and its layers\n",
    "import keras\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define the hyperparameters\n",
    "lstm_units = 64 # The number of units in the LSTM layer\n",
    "dense_units = 24 # The number of units in the first dense layer\n",
    "l2_lambda = 0.01 # The L2 regularization factor\n",
    "learning_rate = 0.01 # The learning rate for the optimizer\n",
    "\n",
    "# Create the sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "\n",
    "# Add the bidirectional LSTM layer with dropout and recurrent dropout\n",
    "model.add(Bidirectional(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "# Add the first dense layer with relu activation and batch normalization\n",
    "model.add(Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add the second dense layer with linear activation\n",
    "model.add(Dense(1, activation='linear', kernel_regularizer=l2(l2_lambda)))\n",
    "\n",
    "# Compile the model with mean squared error loss and RMSprop optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Oriza Sativa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Oriza Sativa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 70)           2100000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               69120     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2172241 (8.29 MB)\n",
      "Trainable params: 2172241 (8.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\Oriza Sativa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Oriza Sativa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.8816\n",
      "Epoch 1: val_accuracy improved from -inf to 0.91423, saving model to C:/Kuliah/Bangkit/Capstone/Sentiment\\test4_model.h5\n",
      "433/433 [==============================] - 44s 95ms/step - loss: 0.2788 - accuracy: 0.8816 - val_loss: 0.2363 - val_accuracy: 0.9142\n",
      "Epoch 2/5\n",
      "  1/433 [..............................] - ETA: 34s - loss: 0.1220 - accuracy: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oriza Sativa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433/433 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9477\n",
      "Epoch 2: val_accuracy did not improve from 0.91423\n",
      "433/433 [==============================] - 39s 90ms/step - loss: 0.1441 - accuracy: 0.9477 - val_loss: 0.2523 - val_accuracy: 0.9071\n",
      "Epoch 3/5\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9686\n",
      "Epoch 3: val_accuracy did not improve from 0.91423\n",
      "433/433 [==============================] - 38s 88ms/step - loss: 0.0935 - accuracy: 0.9686 - val_loss: 0.2745 - val_accuracy: 0.9142\n",
      "Epoch 4/5\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9784\n",
      "Epoch 4: val_accuracy did not improve from 0.91423\n",
      "433/433 [==============================] - 36s 83ms/step - loss: 0.0655 - accuracy: 0.9784 - val_loss: 0.3244 - val_accuracy: 0.9077\n",
      "Epoch 5/5\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9838\n",
      "Epoch 5: val_accuracy did not improve from 0.91423\n",
      "433/433 [==============================] - 35s 82ms/step - loss: 0.0486 - accuracy: 0.9838 - val_loss: 0.3859 - val_accuracy: 0.9045\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "num_epochs = 5\n",
    "checkpoint = ModelCheckpoint('C:/Kuliah/Bangkit/Capstone/Sentiment/test4_model.h5',\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             mode='max',\n",
    "                             verbose=1)\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_split=0.1,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 4s 20ms/step\n",
      "Accuracy of prediction on test set :  0.9028671737858397\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Test sentence:  ui bagus\n",
      "Predicted sentiment:  Positive\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already initialized 'tokenizer' and 'max_length'\n",
    "\n",
    "# Test sentence\n",
    "test_sentence = \"ui bagus\"\n",
    "\n",
    "# Convert the test sentence to a sequence\n",
    "test_sequence = tokenizer.texts_to_sequences([test_sentence])\n",
    "\n",
    "# Pad the sequence\n",
    "padded_test = pad_sequences(test_sequence, padding='post', maxlen=max_length)\n",
    "\n",
    "# Get the label based on probability 1 if p >= 0.5 else 0\n",
    "prediction = model.predict(padded_test)\n",
    "\n",
    "# Convert probability to label\n",
    "pred_label = 1 if prediction >= 0.5 else 0\n",
    "\n",
    "# Display the result\n",
    "print(\"Test sentence: \", test_sentence)\n",
    "print(\"Predicted sentiment: \", \"Positive\" if pred_label == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Memuat model yang telah disimpan\n",
    "loaded_model = load_model('best_model.h5')  # Ganti 'best_model.h5' dengan nama file model yang sesuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(test_labels, pred_labels)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['negatif', 'positif'], yticklabels=['negatif', 'positif'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.ylabel('Aktual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = ' '.join(df['title'].astype(str))\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords='spotify').generate(text_data)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sentiment = df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(count_sentiment, labels=count_sentiment.index, autopct='%1.1f%%', colors=['pink', 'orange'], startangle=90)\n",
    "plt.title('Distribusi Sentimen Hasil Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'at' column to datetime if it's not already in datetime format\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "# Create a new column 'positive' with 1 for positive values and 0 for others\n",
    "df['positive'] = (df['label'] == 'positive').astype(int)\n",
    "\n",
    "# Create a new column 'negative' with 1 for negative values and 0 for others\n",
    "df['negative'] = (df['label'] == 'negative').astype(int)\n",
    "\n",
    "# Assuming that 'label' can have values other than 'positive' and 'negative'\n",
    "# You can set a default value (e.g., 0) for cases where 'label' is neither 'positive' nor 'negative'\n",
    "df['positive'] = df['positive'].where(df['label'].isin(['positive', 'negative']), 0)\n",
    "df['negative'] = df['negative'].where(df['label'].isin(['positive', 'negative']), 0)\n",
    "\n",
    "# Optionally, you can set the 'at' column as the index for a time series\n",
    "df.set_index('at', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot positive values in green\n",
    "ax.plot(at, label, label='Positive', color='green')\n",
    "\n",
    "# Plot negative values in red\n",
    "ax.plot(at, label, label='Negative', color='red')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Positive and Negative Trends in Time Series')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Values')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already initialized 'tokenizer' and 'max_length'\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('[LABELLED]_LinkedIn.csv', sep=',')\n",
    "df = df[[\"text_data\"]]\n",
    "\n",
    "# Convert to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(df[\"text_data\"])\n",
    "\n",
    "# Pad the sequence\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Get labels based on probability 1 if p >= 0.5 else 0\n",
    "prediction = model.predict(padded)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "pred_labels = [1 if p >= 0.5 else 0 for p in prediction]\n",
    "\n",
    "# Create a new DataFrame with the original text data and predicted sentiments\n",
    "result_df = pd.DataFrame({\"text_data\": df[\"text_data\"], \"predicted_sentiment\": pred_labels})\n",
    "\n",
    "# Display predictions\n",
    "for i in range(len(result_df)):\n",
    "    print(result_df[\"text_data\"].iloc[i])\n",
    "    print(\"Predicted sentiment: \", \"Positive\" if result_df[\"predicted_sentiment\"].iloc[i] == 1 else \"Negative\")\n",
    "    print()\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "result_df.to_csv('predicted_sentiments.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
